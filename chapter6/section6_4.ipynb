{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "section6_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zpRXnZ3qcxMl",
        "colab_type": "code",
        "outputId": "d231afd3-67ad-4c38-9193-7ff5ead9789c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        }
      },
      "cell_type": "code",
      "source": [
        "#colabを使う方はこちらを使用ください。\n",
        "!pip install torch==0.4.1\n",
        "!pip install torchvision==0.2.1\n",
        "!pip install numpy==1.14.6\n",
        "!pip install matplotlib==2.1.2\n",
        "!pip install pillow==5.0.0\n",
        "!pip install opencv-python==3.4.3.18"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/a7/6a173738dd6be014ebf9ba6f0b441d91b113b1506a98e10da4ff60994b54/torch-0.4.1-cp27-cp27mu-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 24kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x55c871a86000 @  0x7fd8e041b2a4 0x55c817ebdf18 0x55c817fb1a85 0x55c817ed14ca 0x55c817ed6232 0x55c817eced0a 0x55c817ed65fe 0x55c817eced0a 0x55c817ed65fe 0x55c817eced0a 0x55c817ed65fe 0x55c817eced0a 0x55c817ed6c38 0x55c817eced0a 0x55c817ed65fe 0x55c817eced0a 0x55c817ed65fe 0x55c817ed6232 0x55c817ed6232 0x55c817eced0a 0x55c817ed6c38 0x55c817ed6232 0x55c817eced0a 0x55c817ed6c38 0x55c817eced0a 0x55c817ed6c38 0x55c817eced0a 0x55c817ed65fe 0x55c817eced0a 0x55c817ece629 0x55c817eff61f\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting torchvision==0.2.1\n",
            "  Using cached https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision==0.2.1) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision==0.2.1) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (from torchvision==0.2.1) (0.4.1)\n",
            "Collecting pillow>=4.1.1 (from torchvision==0.2.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/f6/3b3c82c5c75cae471e02fb584136168d732e17ae9db2d21c5dc82f9790f8/Pillow-5.3.0-cp27-cp27mu-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 12.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n",
            "Requirement already satisfied: numpy==1.14.6 in /usr/local/lib/python2.7/dist-packages (1.14.6)\n",
            "Requirement already satisfied: matplotlib==2.1.2 in /usr/local/lib/python2.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (1.14.6)\n",
            "Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (1.5)\n",
            "Requirement already satisfied: subprocess32 in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (3.5.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (2018.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib==2.1.2) (2.3.0)\n",
            "Collecting pillow==5.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/bd/1d9a10f3e8157b7df275740b0782a892a0db387f8286620110c41e5146c7/Pillow-5.0.0-cp27-cp27mu-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.9MB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Found existing installation: Pillow 5.3.0\n",
            "    Uninstalling Pillow-5.3.0:\n",
            "      Successfully uninstalled Pillow-5.3.0\n",
            "Successfully installed pillow-5.0.0\n",
            "Collecting opencv-python==3.4.3.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/3d/9b10d7159e10ac87bd6c1b403dbc72b1240e18da51f9d56f50bb06f75d15/opencv_python-3.4.3.18-cp27-cp27mu-manylinux1_x86_64.whl (25.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 25.0MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from opencv-python==3.4.3.18) (1.14.6)\n",
            "Installing collected packages: opencv-python\n",
            "  Found existing installation: opencv-python 3.4.4.19\n",
            "    Uninstalling opencv-python-3.4.4.19:\n",
            "      Successfully uninstalled opencv-python-3.4.4.19\n",
            "Successfully installed opencv-python-3.4.3.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "11FPu6O9dAjw",
        "colab_type": "code",
        "outputId": "937a5347-f32b-4ec7-b493-0005c5b1b57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#colabを使う方はこちらを使用ください。\n",
        "#Google Driveにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MIOat2A3dH7m",
        "colab_type": "code",
        "outputId": "4e725805-9d6d-4c68-c7de-67c63929ca4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#colabを使う方はこちらを使用ください。※変更の必要がある場合はパスを変更してください。\n",
        "cd /content/gdrive/My Drive/Colab Notebooks/pytorch_handbook/chapter6/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/pytorch_handbook/chapter6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "34z3rH24dKhu",
        "colab_type": "code",
        "outputId": "262c7328-5e0a-46a7-ce64-c75dd863b3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#colabを使う方はこちらを使用ください。\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net.py\t     result_cgan   section6_3.ipynb  train_cgan.py\n",
            "__pycache__  result_lsgan  section6_4.ipynb  train_lsgan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pKAU9MdKEGRu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from net import weights_init, Generator, Discriminator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZDBmvWisEGRy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def onehot_encode(label, device, n_class=10):\n",
        "    \"\"\"\n",
        "    カテゴリカル変数のラベルをOne-Hoe形式に変換する\n",
        "    :param label: 変換対象のラベル\n",
        "    :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
        "    :param n_class: ラベルのクラス数\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    eye = torch.eye(n_class, device=device)\n",
        "    # ランダムベクトルあるいは画像と連結するために(B, c_class, 1, 1)のTensorにして戻す\n",
        "    return eye[label].view(-1, n_class, 1, 1)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GO9cyPyNEGR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def concat_image_label(image, label, device, n_class=10):\n",
        "    \"\"\"\n",
        "    画像とラベルを連結する\n",
        "    :param image:　画像\n",
        "    :param label: ラベル\n",
        "    :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
        "    :param n_class: ラベルのクラス数\n",
        "    :return:　画像とラベルをチャネル方向に連結したTensor\n",
        "    \"\"\"\n",
        "    B, C, H, W = image.shape    # 画像Tensorの大きさを取得\n",
        "    \n",
        "    oh_label = onehot_encode(label, device)         # ラベルをOne-Hotベクトル化\n",
        "    oh_label = oh_label.expand(B, n_class, H, W)    # 画像のサイズに合わせるようラベルを拡張する\n",
        "    return torch.cat((image, oh_label), dim=1)      # 画像とラベルをチャネル方向（dim=1）で連結する"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iVRqLztIEGR3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def concat_noise_label(noise, label, device):\n",
        "    \"\"\"\n",
        "    ノイズ（ランダムベクトル）とラベルを連結する\n",
        "    :param noise: ノイズ\n",
        "    :param label: ラベル\n",
        "    :param device: 学習に使用するデバイス。CPUあるいはGPU\n",
        "    :return:　ノイズとラベルを連結したTensor\n",
        "    \"\"\"\n",
        "    oh_label = onehot_encode(label, device)     # ラベルをOne-Hotベクトル化\n",
        "    return torch.cat((noise, oh_label), dim=1)  # ノイズとラベルをチャネル方向（dim=1）で連結する"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ylkPJXzgEGR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0c76188-6278-4dfa-90eb-ce7a8dbb33d5"
      },
      "cell_type": "code",
      "source": [
        "workers = 2\n",
        "batch_size = 50\n",
        "nz = 100\n",
        "nch_g = 64\n",
        "nch_d = 64\n",
        "n_epoch = 200\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "outf = './result_cgan'\n",
        "display_interval = 100\n",
        "\n",
        "try:\n",
        "    os.makedirs(outf)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fabcff7e250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "TXYnAcifEGR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ddd040eb-84a1-4109-eccd-25234db3a3d4"
      },
      "cell_type": "code",
      "source": [
        "trainset = dset.STL10(root='../../dataset/stl10_root', download=True, split='train',\n",
        "                      transform=transforms.Compose([\n",
        "                          transforms.RandomResizedCrop(64, scale=(88/96, 1.0), ratio=(1., 1.)),\n",
        "                          transforms.RandomHorizontalFlip(),\n",
        "                          transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                      ]))   # ラベルを使用するのでunlabeledを含めない\n",
        "testset = dset.STL10(root='../../dataset/stl10_root', download=True, split='test',\n",
        "                     transform=transforms.Compose([\n",
        "                         transforms.RandomResizedCrop(64, scale=(88/96, 1.0), ratio=(1., 1.)),\n",
        "                         transforms.RandomHorizontalFlip(),\n",
        "                         transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
        "                         transforms.ToTensor(),\n",
        "                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                     ]))\n",
        "dataset = trainset + testset\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=int(workers))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device:', device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZYz-08POEGR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "606641bc-125d-416a-fa83-2bfc97c87d5a"
      },
      "cell_type": "code",
      "source": [
        "# 生成器G。ランダムベクトルとラベルを連結したベクトルから贋作画像を生成する\n",
        "netG = Generator(nz=nz+10, nch_g=nch_g).to(device)   # 入力ベクトルの次元は、ランダムベクトルの次元nzにクラス数10を加算したもの\n",
        "netG.apply(weights_init)\n",
        "print(netG)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (layers): ModuleDict(\n",
            "    (layer0): Sequential(\n",
            "      (0): ConvTranspose2d(110, 512, kernel_size=(4, 4), stride=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layer1): Sequential(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): Tanh()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MSc1GjkIEGSC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "354b1423-6cc1-4287-9fc2-76ba3fa7d2f3"
      },
      "cell_type": "code",
      "source": [
        "# 識別器D。画像とラベルを連結したTensorが、元画像か贋作画像かを識別する\n",
        "netD = Discriminator(nch=3+10, nch_d=nch_d).to(device)   # 入力Tensorのチャネル数は、画像のチャネル数3にクラス数10を加算したもの\n",
        "netD.apply(weights_init)\n",
        "print(netD)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (layers): ModuleDict(\n",
            "    (layer0): Sequential(\n",
            "      (0): Conv2d(13, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer1): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (layer4): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WIOjtuvSEGSG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)\n",
        "\n",
        "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "\n",
        "fixed_label = [i for i in range(10)] * (batch_size // 10)  # 確認用のラベル。0〜9のラベルの繰り返し\n",
        "fixed_label = torch.tensor(fixed_label, dtype=torch.long, device=device)\n",
        "\n",
        "fixed_noise_label = concat_noise_label(fixed_noise, fixed_label, device)  # 確認用のノイズとラベルを連結"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n4unIheuEGSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3605
        },
        "outputId": "dcecd459-a77a-4358-bcfc-1145a51a4af5"
      },
      "cell_type": "code",
      "source": [
        "# 学習のループ\n",
        "for epoch in range(n_epoch):\n",
        "    for itr, data in enumerate(dataloader):\n",
        "        real_image = data[0].to(device)     # 元画像\n",
        "        real_label = data[1].to(device)     # 元画像に対応するラベル\n",
        "        real_image_label = concat_image_label(real_image, real_label, device)   # 元画像とラベルを連結\n",
        "\n",
        "        sample_size = real_image.size(0)\n",
        "        noise = torch.randn(sample_size, nz, 1, 1, device=device)\n",
        "        fake_label = torch.randint(10, (sample_size,), dtype=torch.long, device=device)     # 贋作画像生成用のラベル\n",
        "        fake_noise_label = concat_noise_label(noise, fake_label, device)    # ノイズとラベルを連結\n",
        "        \n",
        "        real_target = torch.full((sample_size,), 1., device=device)\n",
        "        fake_target = torch.full((sample_size,), 0., device=device)\n",
        "\n",
        "        ############################\n",
        "        # 識別器Dの更新\n",
        "        ###########################\n",
        "        netD.zero_grad()\n",
        "\n",
        "        output = netD(real_image_label)     # 識別器Dで元画像とラベルの組み合わせに対する識別信号を出力\n",
        "        errD_real = criterion(output, real_target)        \n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        fake_image = netG(fake_noise_label)     # 生成器Gでラベルに対応した贋作画像を生成\n",
        "        fake_image_label = concat_image_label(fake_image, fake_label, device)   # 贋作画像とラベルを連結\n",
        "\n",
        "        output = netD(fake_image_label.detach())    # 識別器Dで贋作画像とラベルの組み合わせに対する識別信号を出力\n",
        "        errD_fake = criterion(output, fake_target)\n",
        "        D_G_z1 = output.mean().item()\n",
        "\n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # 生成器Gの更新\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        \n",
        "        output = netD(fake_image_label)     # 更新した識別器Dで改めて贋作画像とラベルの組み合わせに対する識別信号を出力\n",
        "        errG = criterion(output, real_target)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        \n",
        "        optimizerG.step()\n",
        "\n",
        "        if itr % display_interval == 0:\n",
        "            print('[{}/{}][{}/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}/{:.3f}'\n",
        "                  .format(epoch + 1, n_epoch,\n",
        "                          itr + 1, len(dataloader),\n",
        "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        if epoch == 0 and itr == 0:\n",
        "            vutils.save_image(real_image, '{}/real_samples.png'.format(outf),\n",
        "                              normalize=True, nrow=10)\n",
        "\n",
        "    ############################\n",
        "    # 確認用画像の生成\n",
        "    ############################\n",
        "    fake_image = netG(fixed_noise_label)    # 1エポック終了ごとに、指定したラベルに対応する贋作画像を生成する\n",
        "    vutils.save_image(fake_image.detach(), '{}/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1),\n",
        "                      normalize=True, nrow=10)\n",
        "\n",
        "    ############################\n",
        "    # モデルの保存\n",
        "    ############################\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        torch.save(netG.state_dict(), '{}/netG_epoch_{}.pth'.format(outf, epoch + 1))\n",
        "        torch.save(netD.state_dict(), '{}/netD_epoch_{}.pth'.format(outf, epoch + 1))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/200][1/260] Loss_D: 1.958 Loss_G: 31.280 D(x): 0.480 D(G(z)): 0.105/1.759\n",
            "[1/200][101/260] Loss_D: 0.378 Loss_G: 0.744 D(x): 0.690 D(G(z)): -0.108/0.208\n",
            "[1/200][201/260] Loss_D: 0.394 Loss_G: 0.652 D(x): 0.660 D(G(z)): 0.315/0.230\n",
            "[2/200][1/260] Loss_D: 0.574 Loss_G: 1.132 D(x): 0.641 D(G(z)): 0.362/-0.007\n",
            "[2/200][101/260] Loss_D: 0.636 Loss_G: 0.553 D(x): 0.544 D(G(z)): 0.403/0.314\n",
            "[2/200][201/260] Loss_D: 0.093 Loss_G: 1.085 D(x): 0.985 D(G(z)): 0.098/-0.027\n",
            "[3/200][1/260] Loss_D: 0.380 Loss_G: 0.811 D(x): 0.604 D(G(z)): 0.200/0.157\n",
            "[3/200][101/260] Loss_D: 0.493 Loss_G: 0.995 D(x): 1.009 D(G(z)): 0.620/0.023\n",
            "[3/200][201/260] Loss_D: 0.486 Loss_G: 0.848 D(x): 0.500 D(G(z)): 0.382/0.087\n",
            "[4/200][1/260] Loss_D: 0.330 Loss_G: 0.462 D(x): 0.572 D(G(z)): 0.242/0.342\n",
            "[4/200][101/260] Loss_D: 0.656 Loss_G: 1.184 D(x): 0.932 D(G(z)): 0.707/-0.054\n",
            "[4/200][201/260] Loss_D: 0.496 Loss_G: 0.732 D(x): 0.848 D(G(z)): 0.571/0.160\n",
            "[5/200][1/260] Loss_D: 0.814 Loss_G: 0.474 D(x): 0.581 D(G(z)): 0.673/0.338\n",
            "[5/200][101/260] Loss_D: 0.888 Loss_G: 1.103 D(x): 0.684 D(G(z)): 0.755/-0.033\n",
            "[5/200][201/260] Loss_D: 0.485 Loss_G: 0.670 D(x): 0.623 D(G(z)): 0.453/0.222\n",
            "[6/200][1/260] Loss_D: 0.342 Loss_G: 1.023 D(x): 0.851 D(G(z)): 0.453/0.008\n",
            "[6/200][101/260] Loss_D: 0.345 Loss_G: 0.652 D(x): 0.572 D(G(z)): 0.281/0.207\n",
            "[6/200][201/260] Loss_D: 0.480 Loss_G: 0.410 D(x): 0.410 D(G(z)): 0.201/0.389\n",
            "[7/200][1/260] Loss_D: 0.445 Loss_G: 0.789 D(x): 0.759 D(G(z)): 0.536/0.136\n",
            "[7/200][101/260] Loss_D: 0.458 Loss_G: 0.570 D(x): 0.571 D(G(z)): 0.470/0.260\n",
            "[7/200][201/260] Loss_D: 0.307 Loss_G: 0.890 D(x): 0.727 D(G(z)): 0.373/0.082\n",
            "[8/200][1/260] Loss_D: 0.457 Loss_G: 0.543 D(x): 0.497 D(G(z)): 0.182/0.318\n",
            "[8/200][101/260] Loss_D: 0.432 Loss_G: 0.584 D(x): 0.686 D(G(z)): 0.500/0.262\n",
            "[8/200][201/260] Loss_D: 0.437 Loss_G: 0.616 D(x): 0.647 D(G(z)): 0.412/0.256\n",
            "[9/200][1/260] Loss_D: 0.497 Loss_G: 0.726 D(x): 0.520 D(G(z)): 0.320/0.187\n",
            "[9/200][101/260] Loss_D: 0.548 Loss_G: 0.384 D(x): 0.405 D(G(z)): 0.380/0.400\n",
            "[9/200][201/260] Loss_D: 0.258 Loss_G: 0.829 D(x): 0.701 D(G(z)): 0.321/0.101\n",
            "[10/200][1/260] Loss_D: 0.523 Loss_G: 0.766 D(x): 0.611 D(G(z)): 0.494/0.154\n",
            "[10/200][101/260] Loss_D: 0.382 Loss_G: 0.735 D(x): 0.652 D(G(z)): 0.408/0.161\n",
            "[10/200][201/260] Loss_D: 0.455 Loss_G: 0.473 D(x): 0.556 D(G(z)): 0.305/0.345\n",
            "[11/200][1/260] Loss_D: 0.286 Loss_G: 0.700 D(x): 0.787 D(G(z)): 0.417/0.178\n",
            "[11/200][101/260] Loss_D: 0.347 Loss_G: 0.504 D(x): 0.703 D(G(z)): 0.438/0.304\n",
            "[11/200][201/260] Loss_D: 0.611 Loss_G: 0.355 D(x): 0.424 D(G(z)): 0.387/0.432\n",
            "[12/200][1/260] Loss_D: 0.394 Loss_G: 0.462 D(x): 0.544 D(G(z)): 0.362/0.341\n",
            "[12/200][101/260] Loss_D: 0.637 Loss_G: 0.637 D(x): 0.683 D(G(z)): 0.616/0.264\n",
            "[12/200][201/260] Loss_D: 0.340 Loss_G: 0.525 D(x): 0.628 D(G(z)): 0.406/0.290\n",
            "[13/200][1/260] Loss_D: 0.429 Loss_G: 0.516 D(x): 0.655 D(G(z)): 0.507/0.298\n",
            "[13/200][101/260] Loss_D: 0.497 Loss_G: 0.416 D(x): 0.570 D(G(z)): 0.512/0.366\n",
            "[13/200][201/260] Loss_D: 0.504 Loss_G: 0.306 D(x): 0.428 D(G(z)): 0.365/0.468\n",
            "[14/200][1/260] Loss_D: 0.315 Loss_G: 0.595 D(x): 0.629 D(G(z)): 0.366/0.241\n",
            "[14/200][101/260] Loss_D: 0.611 Loss_G: 0.333 D(x): 0.322 D(G(z)): 0.348/0.435\n",
            "[14/200][201/260] Loss_D: 0.333 Loss_G: 0.414 D(x): 0.696 D(G(z)): 0.450/0.374\n",
            "[15/200][1/260] Loss_D: 0.532 Loss_G: 0.336 D(x): 0.617 D(G(z)): 0.532/0.453\n",
            "[15/200][101/260] Loss_D: 0.460 Loss_G: 0.586 D(x): 0.449 D(G(z)): 0.283/0.267\n",
            "[15/200][201/260] Loss_D: 0.629 Loss_G: 0.855 D(x): 0.454 D(G(z)): 0.301/0.121\n",
            "[16/200][1/260] Loss_D: 0.480 Loss_G: 0.355 D(x): 0.580 D(G(z)): 0.502/0.425\n",
            "[16/200][101/260] Loss_D: 0.382 Loss_G: 1.438 D(x): 1.017 D(G(z)): 0.510/-0.172\n",
            "[16/200][201/260] Loss_D: 0.538 Loss_G: 0.341 D(x): 0.543 D(G(z)): 0.520/0.452\n",
            "[17/200][1/260] Loss_D: 0.453 Loss_G: 0.880 D(x): 0.883 D(G(z)): 0.549/0.104\n",
            "[17/200][101/260] Loss_D: 0.501 Loss_G: 0.579 D(x): 0.638 D(G(z)): 0.581/0.249\n",
            "[17/200][201/260] Loss_D: 0.539 Loss_G: 0.538 D(x): 0.543 D(G(z)): 0.530/0.275\n",
            "[18/200][1/260] Loss_D: 0.425 Loss_G: 0.502 D(x): 0.568 D(G(z)): 0.438/0.305\n",
            "[18/200][101/260] Loss_D: 0.697 Loss_G: 0.595 D(x): 0.811 D(G(z)): 0.779/0.263\n",
            "[18/200][201/260] Loss_D: 0.510 Loss_G: 0.670 D(x): 0.725 D(G(z)): 0.624/0.191\n",
            "[19/200][1/260] Loss_D: 0.492 Loss_G: 0.298 D(x): 0.415 D(G(z)): 0.267/0.471\n",
            "[19/200][101/260] Loss_D: 0.518 Loss_G: 0.603 D(x): 0.631 D(G(z)): 0.557/0.237\n",
            "[19/200][201/260] Loss_D: 0.374 Loss_G: 0.389 D(x): 0.624 D(G(z)): 0.403/0.404\n",
            "[20/200][1/260] Loss_D: 0.351 Loss_G: 0.397 D(x): 0.619 D(G(z)): 0.394/0.382\n",
            "[20/200][101/260] Loss_D: 0.612 Loss_G: 0.864 D(x): 0.769 D(G(z)): 0.694/0.110\n",
            "[20/200][201/260] Loss_D: 0.452 Loss_G: 0.456 D(x): 0.519 D(G(z)): 0.392/0.339\n",
            "[21/200][1/260] Loss_D: 0.477 Loss_G: 0.477 D(x): 0.480 D(G(z)): 0.405/0.321\n",
            "[21/200][101/260] Loss_D: 0.572 Loss_G: 0.230 D(x): 0.301 D(G(z)): 0.189/0.549\n",
            "[21/200][201/260] Loss_D: 0.465 Loss_G: 0.487 D(x): 0.605 D(G(z)): 0.480/0.311\n",
            "[22/200][1/260] Loss_D: 0.463 Loss_G: 0.540 D(x): 0.623 D(G(z)): 0.525/0.274\n",
            "[22/200][101/260] Loss_D: 0.405 Loss_G: 0.380 D(x): 0.504 D(G(z)): 0.352/0.395\n",
            "[22/200][201/260] Loss_D: 0.351 Loss_G: 0.417 D(x): 0.573 D(G(z)): 0.341/0.368\n",
            "[23/200][1/260] Loss_D: 0.593 Loss_G: 0.603 D(x): 0.640 D(G(z)): 0.478/0.250\n",
            "[23/200][101/260] Loss_D: 0.414 Loss_G: 0.519 D(x): 0.452 D(G(z)): 0.285/0.287\n",
            "[23/200][201/260] Loss_D: 0.529 Loss_G: 0.369 D(x): 0.431 D(G(z)): 0.421/0.400\n",
            "[24/200][1/260] Loss_D: 0.443 Loss_G: 0.372 D(x): 0.650 D(G(z)): 0.503/0.412\n",
            "[24/200][101/260] Loss_D: 0.476 Loss_G: 0.389 D(x): 0.591 D(G(z)): 0.524/0.385\n",
            "[24/200][201/260] Loss_D: 0.311 Loss_G: 0.655 D(x): 0.730 D(G(z)): 0.442/0.198\n",
            "[25/200][1/260] Loss_D: 0.315 Loss_G: 0.891 D(x): 0.725 D(G(z)): 0.451/0.062\n",
            "[25/200][101/260] Loss_D: 0.557 Loss_G: 0.312 D(x): 0.343 D(G(z)): 0.234/0.460\n",
            "[25/200][201/260] Loss_D: 0.795 Loss_G: 1.151 D(x): 1.099 D(G(z)): 0.849/-0.065\n",
            "[26/200][1/260] Loss_D: 0.354 Loss_G: 0.625 D(x): 0.656 D(G(z)): 0.442/0.223\n",
            "[26/200][101/260] Loss_D: 0.633 Loss_G: 0.584 D(x): 0.695 D(G(z)): 0.660/0.262\n",
            "[26/200][201/260] Loss_D: 0.460 Loss_G: 0.627 D(x): 0.746 D(G(z)): 0.575/0.223\n",
            "[27/200][1/260] Loss_D: 0.366 Loss_G: 0.504 D(x): 0.502 D(G(z)): 0.265/0.304\n",
            "[27/200][101/260] Loss_D: 0.360 Loss_G: 0.662 D(x): 0.762 D(G(z)): 0.505/0.201\n",
            "[27/200][201/260] Loss_D: 0.472 Loss_G: 0.427 D(x): 0.588 D(G(z)): 0.490/0.371\n",
            "[28/200][1/260] Loss_D: 0.424 Loss_G: 0.408 D(x): 0.427 D(G(z)): 0.209/0.383\n",
            "[28/200][101/260] Loss_D: 0.341 Loss_G: 0.764 D(x): 0.825 D(G(z)): 0.500/0.140\n",
            "[28/200][201/260] Loss_D: 0.340 Loss_G: 0.862 D(x): 0.759 D(G(z)): 0.480/0.091\n",
            "[29/200][1/260] Loss_D: 0.543 Loss_G: 0.444 D(x): 0.631 D(G(z)): 0.601/0.349\n",
            "[29/200][101/260] Loss_D: 0.397 Loss_G: 0.336 D(x): 0.516 D(G(z)): 0.288/0.447\n",
            "[29/200][201/260] Loss_D: 0.428 Loss_G: 0.883 D(x): 0.757 D(G(z)): 0.558/0.071\n",
            "[30/200][1/260] Loss_D: 0.424 Loss_G: 0.836 D(x): 0.696 D(G(z)): 0.546/0.094\n",
            "[30/200][101/260] Loss_D: 0.362 Loss_G: 0.704 D(x): 0.643 D(G(z)): 0.441/0.175\n",
            "[30/200][201/260] Loss_D: 0.367 Loss_G: 0.620 D(x): 0.529 D(G(z)): 0.292/0.238\n",
            "[31/200][1/260] Loss_D: 0.397 Loss_G: 0.503 D(x): 0.640 D(G(z)): 0.469/0.306\n",
            "[31/200][101/260] Loss_D: 0.305 Loss_G: 0.946 D(x): 0.853 D(G(z)): 0.469/0.035\n",
            "[31/200][201/260] Loss_D: 0.644 Loss_G: 1.556 D(x): 0.970 D(G(z)): 0.765/-0.238\n",
            "[32/200][1/260] Loss_D: 0.317 Loss_G: 0.950 D(x): 0.769 D(G(z)): 0.466/0.035\n",
            "[32/200][101/260] Loss_D: 0.434 Loss_G: 0.850 D(x): 0.647 D(G(z)): 0.468/0.090\n",
            "[32/200][201/260] Loss_D: 0.535 Loss_G: 0.337 D(x): 0.343 D(G(z)): 0.135/0.435\n",
            "[33/200][1/260] Loss_D: 0.458 Loss_G: 0.898 D(x): 0.883 D(G(z)): 0.626/0.063\n",
            "[33/200][101/260] Loss_D: 0.891 Loss_G: 3.846 D(x): 0.696 D(G(z)): 0.823/-0.940\n",
            "[33/200][201/260] Loss_D: 0.480 Loss_G: 1.200 D(x): 0.889 D(G(z)): 0.634/-0.083\n",
            "[34/200][1/260] Loss_D: 0.329 Loss_G: 0.946 D(x): 0.802 D(G(z)): 0.469/0.040\n",
            "[34/200][101/260] Loss_D: 0.451 Loss_G: 0.446 D(x): 0.478 D(G(z)): 0.352/0.346\n",
            "[34/200][201/260] Loss_D: 0.439 Loss_G: 0.474 D(x): 0.499 D(G(z)): 0.350/0.329\n",
            "[35/200][1/260] Loss_D: 0.378 Loss_G: 0.736 D(x): 0.630 D(G(z)): 0.303/0.178\n",
            "[35/200][101/260] Loss_D: 0.276 Loss_G: 0.437 D(x): 0.530 D(G(z)): 0.067/0.357\n",
            "[35/200][201/260] Loss_D: 0.354 Loss_G: 0.766 D(x): 0.816 D(G(z)): 0.501/0.135\n",
            "[36/200][1/260] Loss_D: 0.470 Loss_G: 0.287 D(x): 0.359 D(G(z)): 0.041/0.489\n",
            "[36/200][101/260] Loss_D: 0.403 Loss_G: 0.617 D(x): 0.623 D(G(z)): 0.432/0.234\n",
            "[36/200][201/260] Loss_D: 0.239 Loss_G: 0.602 D(x): 0.654 D(G(z)): 0.294/0.235\n",
            "[37/200][1/260] Loss_D: 0.355 Loss_G: 0.491 D(x): 0.529 D(G(z)): 0.279/0.316\n",
            "[37/200][101/260] Loss_D: 0.311 Loss_G: 0.405 D(x): 0.539 D(G(z)): 0.229/0.376\n",
            "[37/200][201/260] Loss_D: 0.298 Loss_G: 0.565 D(x): 0.765 D(G(z)): 0.407/0.263\n",
            "[38/200][1/260] Loss_D: 0.353 Loss_G: 0.625 D(x): 0.745 D(G(z)): 0.500/0.220\n",
            "[38/200][101/260] Loss_D: 0.328 Loss_G: 0.807 D(x): 0.665 D(G(z)): 0.262/0.145\n",
            "[38/200][201/260] Loss_D: 0.230 Loss_G: 0.826 D(x): 0.696 D(G(z)): 0.261/0.103\n",
            "[39/200][1/260] Loss_D: 0.413 Loss_G: 0.727 D(x): 0.724 D(G(z)): 0.525/0.179\n",
            "[39/200][101/260] Loss_D: 0.277 Loss_G: 0.561 D(x): 0.676 D(G(z)): 0.319/0.268\n",
            "[39/200][201/260] Loss_D: 0.305 Loss_G: 0.538 D(x): 0.557 D(G(z)): 0.188/0.286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-77:\n",
            "Process Process-78:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
            "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
            "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
            "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\", line 81, in __getitem__\n",
            "    return self.datasets[dataset_idx][sample_idx]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/stl10.py\", line 108, in __getitem__\n",
            "    img = self.transform(img)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
            "    r = wait([self], timeout)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
            "    img = t(img)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 756, in __call__\n",
            "    return transform(img)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
            "    img = t(img)\n",
            "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
            "    fd_event_list = self._poll.poll(timeout)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 283, in __call__\n",
            "    return self.lambd(img)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 739, in <lambda>\n",
            "    transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 535, in adjust_hue\n",
            "    img = Image.merge('HSV', (h, s, v)).convert(input_mode)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 986, in convert\n",
            "    new_im = self._new(im)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 540, in _new\n",
            "    new = Image()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 527, in __init__\n",
            "    self.category = NORMAL\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2ee600b0eea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_image_label\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# 更新した識別器Dで改めて贋作画像とラベルの組み合わせに対する識別信号を出力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pointwise_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, reduction)\u001b[0m\n\u001b[1;32m   1672\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'elementwise_mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "D1PemeWySvW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}